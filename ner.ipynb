{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "import csv\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first thing to do is to get the data ready\n",
    "\n",
    "#first let us get our tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2', do_lower_case=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "1. We read the tsv file \n",
    "2. Break corpus into sentences and tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceFetch(object):\n",
    "  \n",
    "  def __init__(self, data):\n",
    "    self.data = data\n",
    "    self.sentences = []\n",
    "    self.tags = []\n",
    "    self.sent = []\n",
    "    self.tag = []\n",
    "    \n",
    "    # make tsv file readable\n",
    "    with open(self.data) as tsv_f:\n",
    "      reader = csv.reader(tsv_f, delimiter='\\t')\n",
    "      for row in reader:\n",
    "        if len(row) == 0:\n",
    "          if len(self.sent) != len(self.tag):\n",
    "            break\n",
    "          self.sentences.append(self.sent)\n",
    "          self.tags.append(self.tag)\n",
    "          self.sent = []\n",
    "          self.tag = []\n",
    "        else:\n",
    "          self.sent.append(row[0])\n",
    "          self.tag.append(row[1])   \n",
    "\n",
    "  def getSentences(self):\n",
    "    return self.sentences\n",
    "  \n",
    "  def getTags(self):\n",
    "    return self.tags\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  7462\n",
      "Number of samples:  1448\n",
      "Number of samples:  2446\n"
     ]
    }
   ],
   "source": [
    "def extractFromDirectories(parent_dir, corpus_path, file_type = 'train.tsv'):\n",
    "    sentences, tags = [], []\n",
    "    for path, dirs, files in os.walk(parent_dir):\n",
    "        for file in files:\n",
    "            if path == corpus_path:\n",
    "                if file == file_type:\n",
    "                    current_path = os.path.join(path,file)\n",
    "                    sentence = SentenceFetch(current_path).getSentences()\n",
    "                    tag = SentenceFetch(current_path).getTags()\n",
    "                    sentences.extend(sentence)\n",
    "                    tags.extend(tag)\n",
    "    \n",
    "    print('Number of samples: ',len(sentences))\n",
    "\n",
    "    return sentences, tags\n",
    "\n",
    "PARENT_DIR = 'BioNLP'\n",
    "CORPUS_PATH = 'BioNLP\\BioNLP09-IOB'\n",
    "\n",
    "train_sentences, train_tags = extractFromDirectories(parent_dir=PARENT_DIR,\n",
    "                                                     corpus_path=CORPUS_PATH,\n",
    "                                                     file_type='train.tsv')\n",
    "\n",
    "val_sentences, val_tags = extractFromDirectories(parent_dir=PARENT_DIR,\n",
    "                                                     corpus_path=CORPUS_PATH,\n",
    "                                                     file_type='devel.tsv')\n",
    "\n",
    "test_sentences, test_tags = extractFromDirectories(parent_dir=PARENT_DIR,\n",
    "                                                     corpus_path=CORPUS_PATH,\n",
    "                                                     file_type='test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reactive',\n",
       " 'oxygen',\n",
       " 'intermediate',\n",
       " '-',\n",
       " 'dependent',\n",
       " 'NF',\n",
       " '-',\n",
       " 'kappaB',\n",
       " 'activation',\n",
       " 'by',\n",
       " 'interleukin',\n",
       " '-',\n",
       " '1beta',\n",
       " 'requires',\n",
       " '5',\n",
       " '-',\n",
       " 'lipoxygenase',\n",
       " 'or',\n",
       " 'NADPH',\n",
       " 'oxidase',\n",
       " 'activity',\n",
       " '.']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sent = tokenizer.encode_plus(train_sentences[0], is_split_into_words=True, truncation=True, padding = 'max_length', max_length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 11336, 19667, 7621, 9533, 118, 7449, 151, 2271, 118, 24181, 13059, 2064, 14915, 1118, 9455, 1513, 17041, 1179, 118, 122, 16632, 1161, 5315, 126, 118, 4764, 10649, 1183, 4915, 6530, 1137, 151, 14569, 2101, 3048, 184, 8745, 9028, 1162, 3246, 119, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sent['input_ids'].tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Re',\n",
       " '##active',\n",
       " 'oxygen',\n",
       " 'intermediate',\n",
       " '-',\n",
       " 'dependent',\n",
       " 'N',\n",
       " '##F',\n",
       " '-',\n",
       " 'ka',\n",
       " '##ppa',\n",
       " '##B',\n",
       " 'activation',\n",
       " 'by',\n",
       " 'inter',\n",
       " '##le',\n",
       " '##uki',\n",
       " '##n',\n",
       " '-',\n",
       " '1',\n",
       " '##bet',\n",
       " '##a',\n",
       " 'requires',\n",
       " '5',\n",
       " '-',\n",
       " 'lip',\n",
       " '##ox',\n",
       " '##y',\n",
       " '##gen',\n",
       " '##ase',\n",
       " 'or',\n",
       " 'N',\n",
       " '##AD',\n",
       " '##P',\n",
       " '##H',\n",
       " 'o',\n",
       " '##xi',\n",
       " '##das',\n",
       " '##e',\n",
       " 'activity',\n",
       " '.',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoded_sent['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101,\n",
       "  11336,\n",
       "  19667,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  7621,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  9533,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  118,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  7449,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  151,\n",
       "  2271,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  118,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  24181,\n",
       "  13059,\n",
       "  2064,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  14915,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  1118,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  9455,\n",
       "  1513,\n",
       "  17041,\n",
       "  1179,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  118,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  122,\n",
       "  16632,\n",
       "  1161,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  5315,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  126,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  118,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  4764,\n",
       "  10649,\n",
       "  1183,\n",
       "  4915,\n",
       "  6530,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  1137,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  151,\n",
       "  14569,\n",
       "  2101,\n",
       "  3048,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  184,\n",
       "  8745,\n",
       "  9028,\n",
       "  1162,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  3246,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  119,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentence = tokenizer(train_sentences[0], padding = 'max_length', max_length = 50, truncation=True)\n",
    "\n",
    "encoded_sentence['input_ids']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing data and keeping labels intact\n",
    "\n",
    "1. Now we zip sentences and tags \n",
    "2. We tokenize each word; note that some words are broken into sub-words.\n",
    "3. To deal with that, we just extend the label to all subwords generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import itertools\n",
    "\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "def tokenizeAndLabelSample(sentence, text_labels):\n",
    "  \"\"\"Generate tokens for words in a text sequence while keeping labels intact. This function does tokenization on the sentence level\"\"\"\n",
    "\n",
    "  tokens = []\n",
    "  labels = []\n",
    "\n",
    "  #zip sentence and label\n",
    "  for word, label in zip(sentence, text_labels):\n",
    "    token = tokenizer.tokenize(word)\n",
    "    tokens.extend(token)\n",
    "\n",
    "    #make sure label is copied for each sub-word IF tokenizer generates sub-words for given word\n",
    "    #len(token) is > 1 if sub-words generated\n",
    "    labels.extend([label]*len(token)) \n",
    "  \n",
    "  return tokens, labels\n",
    "\n",
    "def tokenizeDataset(sentences, labels):\n",
    "  \"\"\"This funciton uses tokenizeAndLabelSample and runs it on the entire dataset\"\"\"\n",
    "  tokenized_text = []\n",
    "  text_labels = []\n",
    "\n",
    "  for i in range(len(sentences)):\n",
    "    sent_tokens, sent_labels = tokenizeAndLabelSample(sentences[i],labels[i])\n",
    "    tokenized_text.append(sent_tokens)\n",
    "    text_labels.append(sent_labels)\n",
    "  \n",
    "  return tokenized_text, text_labels\n",
    "\n",
    "def generateInputIds(tokenized_text):\n",
    "  \"\"\"Generate input_ids\"\"\"\n",
    "  input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(text_sequence) for text_sequence in tokenized_text],\n",
    "                            maxlen = MAX_LENGTH, dtype='long',value=0.0, truncating='post',padding='post')\n",
    "  \n",
    "  return input_ids\n",
    "\n",
    "def generateAttentionMask(input_ids):\n",
    "  \"\"\"Generates attention mask for input ids\"\"\"\n",
    "  attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "\n",
    "  return attention_masks\n",
    "\n",
    "def alignLabels(tags, text_labels):\n",
    "  \"\"\"Ensures that labels match \"\"\"\n",
    "  #get all unique tag values\n",
    "  tag_values = list(set(itertools.chain.from_iterable(tags)))\n",
    "  #append PAD token to tag_values\n",
    "  tag_values.append(\"PAD\")\n",
    "  #create a dictionary mapping tag values to ids\n",
    "  tag_id_dict = {t: i for i,t in enumerate(tag_values)}\n",
    "  #pad \n",
    "  tag_ids = pad_sequences([[tag_id_dict.get(l) for l in lab] for lab in text_labels],\n",
    "                     maxlen=MAX_LENGTH, value=tag_id_dict[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "  \n",
    "  return tag_ids, tag_id_dict\n",
    "\n",
    "def generateInputs(sentences, tags):\n",
    "  \"\"\"A wrapped function that does the whole processing of tokenization and aligning of labels\"\"\"\n",
    "\n",
    "  #tokenize dataset\n",
    "  tokenized_text, text_labels = tokenizeDataset(sentences, tags)\n",
    "\n",
    "  #generate input ids\n",
    "  input_ids = generateInputIds(tokenized_text)\n",
    "\n",
    "  #generate attention mask\n",
    "  attention_masks = generateAttentionMask(input_ids)\n",
    "\n",
    "  #align labels\n",
    "  tag_ids, tag_id_dict = alignLabels(tags, text_labels)\n",
    "\n",
    "  return input_ids, attention_masks, tag_ids, tag_id_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_attention_masks, train_tag_ids, train_tag_id_dict = generateInputs(train_sentences,train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11336, 19667,  7621,  9533,   118,  7449,   151,  2271,   118,\n",
       "       24181, 13059,  2064, 14915,  1118,  9455,  1513, 17041,  1179,\n",
       "         118,   122, 16632,  1161,  5315,   126,   118,  4764, 10649,\n",
       "        1183,  4915,  6530,  1137,   151, 14569,  2101,  3048,   184,\n",
       "        8745,  9028,  1162,  3246])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text, text_labels = tokenizeDataset(sentences,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets get our input ids \n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import itertools\n",
    "\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(text_sequence) for text_sequence in tokenized_text],\n",
    "                            maxlen = 100, dtype='long',value=0.0, truncating='post',padding='post')\n",
    "\n",
    "tag_values = list(set(itertools.chain.from_iterable(tags)))\n",
    "\n",
    "tag_values.append(\"PAD\")\n",
    "\n",
    "tag2idx = {t: i for i,t in enumerate(tag_values)}\n",
    "\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in text_labels],\n",
    "                     maxlen=100, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate attendtion masks\n",
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tags[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "tr_inputs = torch.tensor(input_ids)\n",
    "tr_tags = torch.tensor(tags)\n",
    "tr_masks = torch.tensor(attention_masks)\n",
    "\n",
    "train_dataset = TensorDataset(tr_inputs,tr_masks,tr_tags)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler = RandomSampler(train_dataset),\n",
    "    batch_size = BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'I-Protein', 'E-Protein', 'S-Protein', 'B-Protein', 'PAD']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\adil.ahmed\\Miniconda3\\envs\\biobert\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, AdamW\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.2', num_labels = len(tag_values))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "#check for cuda\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "# for num, batch in enumerate(train_loader):\n",
    "#     label = batch[2].type(torch.LongTensor).to(device)\n",
    "\n",
    "#     output = model(input_ids = batch[0].to(device), attention_mask = batch[1].to(device), labels = label)\n",
    "#     loss = output[0]\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 6 ========\n",
      "Training...\n",
      "Training loss:  0.11376170058101416\n",
      "\n",
      "======== Epoch 2 / 6 ========\n",
      "Training...\n",
      "Training loss:  0.04043763983175158\n",
      "\n",
      "======== Epoch 3 / 6 ========\n",
      "Training...\n",
      "Training loss:  0.03095139863193035\n",
      "\n",
      "======== Epoch 4 / 6 ========\n",
      "Training...\n",
      "Training loss:  0.02555634008385241\n",
      "\n",
      "======== Epoch 5 / 6 ========\n",
      "Training...\n",
      "Training loss:  0.021814862606860698\n",
      "\n",
      "======== Epoch 6 / 6 ========\n",
      "Training...\n",
      "Training loss:  0.01945531152449548\n"
     ]
    }
   ],
   "source": [
    "#specify number of epochs\n",
    "epochs = 6\n",
    "\n",
    "total_steps = len(train_loader)*epochs\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "\n",
    "for epoch_i in range(0,epochs):\n",
    "\n",
    "    #keeping track of ep\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    #flush out total loss after each epoch\n",
    "    total_train_loss = 0\n",
    "\n",
    "    #put model in training mode\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_label = batch[2].type(torch.LongTensor).to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(input_ids = batch_input_ids,\n",
    "                       attention_mask = batch_attention_mask,\n",
    "                       labels = batch_label)\n",
    "        \n",
    "        loss = output[0]\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        #backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        #clip the norm of gradients to 1.0\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    average_train_loss = total_train_loss / len(train_loader)\n",
    "    print('Training loss: ',average_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/tokenizer_config.json',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/vocab.txt',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the model\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model and tokenizer\n",
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "\n",
    "input_dir = './model_save/'\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(input_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(input_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adil.ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "#Using the model for predictions\n",
    "text = \"\"\"In addition to their essential catalytic role in protein biosynthesis, aminoacyl-tRNA synthetases participate in numerous other functions, including regulation of gene expression and amino acid biosynthesis via transamidation pathways. Herein, we describe a class of aminoacyl-tRNA synthetase-like (HisZ) proteins based on the catalytic core of the contemporary class II histidyl-tRNA synthetase whose members lack aminoacylation activity but are instead essential components of the first enzyme in histidine biosynthesis ATP phosphoribosyltransferase (HisG). Prediction of the function of HisZ in Lactococcus lactis was assisted by comparative genomics, a technique that revealed a link between the presence or the absence of HisZ and a systematic variation in the length of the HisG polypeptide. HisZ is required for histidine prototrophy, and three other lines of evidence support the direct involvement of HisZ in the transferase function. (i) Genetic experiments demonstrate that complementation of an in-frame deletion of HisG from Escherichia coli (which does not possess HisZ) requires both HisG and HisZ from L. lactis. (ii) Coelution of HisG and HisZ during affinity chromatography provides evidence of direct physical interaction. (iii) Both HisG and HisZ are required for catalysis of the ATP phosphoribosyltransferase reaction. This observation of a common protein domain linking amino acid biosynthesis and protein synthesis implies an early connection between the biosynthesis of amino acids and proteins.\"\"\"\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "sent_text = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In addition to their essential catalytic role in protein biosynthesis, aminoacyl-tRNA synthetases participate in numerous other functions, including regulation of gene expression and amino acid biosynthesis via transamidation pathways.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = []\n",
    "for sentence in sent_text:\n",
    "    tokenized_text.append(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['In',\n",
       "  'addition',\n",
       "  'to',\n",
       "  'their',\n",
       "  'essential',\n",
       "  'catalytic',\n",
       "  'role',\n",
       "  'in',\n",
       "  'protein',\n",
       "  'biosynthesis',\n",
       "  ',',\n",
       "  'aminoacyl-tRNA',\n",
       "  'synthetases',\n",
       "  'participate',\n",
       "  'in',\n",
       "  'numerous',\n",
       "  'other',\n",
       "  'functions',\n",
       "  ',',\n",
       "  'including',\n",
       "  'regulation',\n",
       "  'of',\n",
       "  'gene',\n",
       "  'expression',\n",
       "  'and',\n",
       "  'amino',\n",
       "  'acid',\n",
       "  'biosynthesis',\n",
       "  'via',\n",
       "  'transamidation',\n",
       "  'pathways',\n",
       "  '.'],\n",
       " ['Herein',\n",
       "  ',',\n",
       "  'we',\n",
       "  'describe',\n",
       "  'a',\n",
       "  'class',\n",
       "  'of',\n",
       "  'aminoacyl-tRNA',\n",
       "  'synthetase-like',\n",
       "  '(',\n",
       "  'HisZ',\n",
       "  ')',\n",
       "  'proteins',\n",
       "  'based',\n",
       "  'on',\n",
       "  'the',\n",
       "  'catalytic',\n",
       "  'core',\n",
       "  'of',\n",
       "  'the',\n",
       "  'contemporary',\n",
       "  'class',\n",
       "  'II',\n",
       "  'histidyl-tRNA',\n",
       "  'synthetase',\n",
       "  'whose',\n",
       "  'members',\n",
       "  'lack',\n",
       "  'aminoacylation',\n",
       "  'activity',\n",
       "  'but',\n",
       "  'are',\n",
       "  'instead',\n",
       "  'essential',\n",
       "  'components',\n",
       "  'of',\n",
       "  'the',\n",
       "  'first',\n",
       "  'enzyme',\n",
       "  'in',\n",
       "  'histidine',\n",
       "  'biosynthesis',\n",
       "  'ATP',\n",
       "  'phosphoribosyltransferase',\n",
       "  '(',\n",
       "  'HisG',\n",
       "  ')',\n",
       "  '.'],\n",
       " ['Prediction',\n",
       "  'of',\n",
       "  'the',\n",
       "  'function',\n",
       "  'of',\n",
       "  'HisZ',\n",
       "  'in',\n",
       "  'Lactococcus',\n",
       "  'lactis',\n",
       "  'was',\n",
       "  'assisted',\n",
       "  'by',\n",
       "  'comparative',\n",
       "  'genomics',\n",
       "  ',',\n",
       "  'a',\n",
       "  'technique',\n",
       "  'that',\n",
       "  'revealed',\n",
       "  'a',\n",
       "  'link',\n",
       "  'between',\n",
       "  'the',\n",
       "  'presence',\n",
       "  'or',\n",
       "  'the',\n",
       "  'absence',\n",
       "  'of',\n",
       "  'HisZ',\n",
       "  'and',\n",
       "  'a',\n",
       "  'systematic',\n",
       "  'variation',\n",
       "  'in',\n",
       "  'the',\n",
       "  'length',\n",
       "  'of',\n",
       "  'the',\n",
       "  'HisG',\n",
       "  'polypeptide',\n",
       "  '.'],\n",
       " ['HisZ',\n",
       "  'is',\n",
       "  'required',\n",
       "  'for',\n",
       "  'histidine',\n",
       "  'prototrophy',\n",
       "  ',',\n",
       "  'and',\n",
       "  'three',\n",
       "  'other',\n",
       "  'lines',\n",
       "  'of',\n",
       "  'evidence',\n",
       "  'support',\n",
       "  'the',\n",
       "  'direct',\n",
       "  'involvement',\n",
       "  'of',\n",
       "  'HisZ',\n",
       "  'in',\n",
       "  'the',\n",
       "  'transferase',\n",
       "  'function',\n",
       "  '.'],\n",
       " ['(',\n",
       "  'i',\n",
       "  ')',\n",
       "  'Genetic',\n",
       "  'experiments',\n",
       "  'demonstrate',\n",
       "  'that',\n",
       "  'complementation',\n",
       "  'of',\n",
       "  'an',\n",
       "  'in-frame',\n",
       "  'deletion',\n",
       "  'of',\n",
       "  'HisG',\n",
       "  'from',\n",
       "  'Escherichia',\n",
       "  'coli',\n",
       "  '(',\n",
       "  'which',\n",
       "  'does',\n",
       "  'not',\n",
       "  'possess',\n",
       "  'HisZ',\n",
       "  ')',\n",
       "  'requires',\n",
       "  'both',\n",
       "  'HisG',\n",
       "  'and',\n",
       "  'HisZ',\n",
       "  'from',\n",
       "  'L.',\n",
       "  'lactis',\n",
       "  '.'],\n",
       " ['(',\n",
       "  'ii',\n",
       "  ')',\n",
       "  'Coelution',\n",
       "  'of',\n",
       "  'HisG',\n",
       "  'and',\n",
       "  'HisZ',\n",
       "  'during',\n",
       "  'affinity',\n",
       "  'chromatography',\n",
       "  'provides',\n",
       "  'evidence',\n",
       "  'of',\n",
       "  'direct',\n",
       "  'physical',\n",
       "  'interaction',\n",
       "  '.'],\n",
       " ['(',\n",
       "  'iii',\n",
       "  ')',\n",
       "  'Both',\n",
       "  'HisG',\n",
       "  'and',\n",
       "  'HisZ',\n",
       "  'are',\n",
       "  'required',\n",
       "  'for',\n",
       "  'catalysis',\n",
       "  'of',\n",
       "  'the',\n",
       "  'ATP',\n",
       "  'phosphoribosyltransferase',\n",
       "  'reaction',\n",
       "  '.'],\n",
       " ['This',\n",
       "  'observation',\n",
       "  'of',\n",
       "  'a',\n",
       "  'common',\n",
       "  'protein',\n",
       "  'domain',\n",
       "  'linking',\n",
       "  'amino',\n",
       "  'acid',\n",
       "  'biosynthesis',\n",
       "  'and',\n",
       "  'protein',\n",
       "  'synthesis',\n",
       "  'implies',\n",
       "  'an',\n",
       "  'early',\n",
       "  'connection',\n",
       "  'between',\n",
       "  'the',\n",
       "  'biosynthesis',\n",
       "  'of',\n",
       "  'amino',\n",
       "  'acids',\n",
       "  'and',\n",
       "  'proteins',\n",
       "  '.']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = tokenized_text[0]\n",
    "\n",
    "len(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = []\n",
    "\n",
    "for word in example:\n",
    "    tokenized_word = tokenizer.tokenize(word)\n",
    "    tokenized_sentence.extend(tokenized_word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "input_attentions = [[1]*len(input_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sentences = []\n",
    "pred_labels = []\n",
    "\n",
    "for x,y in zip(input_ids,input_attentions):\n",
    "    x = torch.tensor(x).cuda()\n",
    "    y = torch.tensor(y).cuda()\n",
    "    #x = x.view(-1,x.size()[-1])\n",
    "    #y = y.view(-1,y.size()[-1])\n",
    "    with torch.no_grad():\n",
    "        _,y_hat = model(x,y,None)\n",
    "    label_indices = y_hat.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = ['The',\n",
    " 'Cdc6',\n",
    " 'protein',\n",
    " 'is',\n",
    " 'ubiquitinated',\n",
    " 'in',\n",
    " 'vivo',\n",
    " 'for',\n",
    " 'proteolysis',\n",
    " 'in',\n",
    " 'Saccharomyces',\n",
    " 'cerevisiae',\n",
    " '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_example = sent_text[0]\n",
    "\n",
    "text = tokenizer(sentences[1], padding = 'max_length', max_length = 100, truncation=True, return_tensors ='pt')\n",
    "\n",
    "input_id = text['input_ids'].to(device)\n",
    "mask = text['attention_mask'].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_id,mask,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output['logits'][0]\n",
    "predictions = logits.argmax(dim=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biobert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Oct 19 2022, 10:19:43) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "769716ad7161839e5762a2bdd5cc31ef635bc5adfd6e920a903d2a775d1f4500"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
