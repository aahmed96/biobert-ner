{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "#import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first thing to do is to get the data ready\n",
    "\n",
    "#first let us get our tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2', do_lower_case=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "1. We read the tsv file \n",
    "2. Break corpus into sentences and tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocessing_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing_functions.py\n",
    "class SentenceFetch(object):\n",
    "  \n",
    "  def __init__(self, data):\n",
    "    self.data = data\n",
    "    self.sentences = []\n",
    "    self.tags = []\n",
    "    self.sent = []\n",
    "    self.tag = []\n",
    "    \n",
    "    # make tsv file readable\n",
    "    with open(self.data) as tsv_f:\n",
    "      reader = csv.reader(tsv_f, delimiter='\\t')\n",
    "      \n",
    "      for row in reader:\n",
    "        if len(row) == 0:\n",
    "          if len(self.sent) != len(self.tag):\n",
    "            break\n",
    "          self.sentences.append(self.sent)\n",
    "          self.tags.append(self.tag)\n",
    "          self.sent = []\n",
    "          self.tag = []\n",
    "        else:\n",
    "          self.sent.append(row[0])\n",
    "          self.tag.append(row[1])   \n",
    "\n",
    "  def getSentences(self):\n",
    "    return self.sentences\n",
    "  \n",
    "  def getTags(self):\n",
    "    return self.tags\n",
    "\n",
    "def extractFromDirectories(parent_dir, corpus_path, file_type = 'train.tsv'):\n",
    "    sentences, tags = [], []\n",
    "\n",
    "    if os.path.exists(os.path.join(parent_dir, corpus_path)):\n",
    "        files = os.listdir(os.path.join(parent_dir, corpus_path))\n",
    "        for file in files:\n",
    "            if file == file_type:\n",
    "                \n",
    "                current_path = os.path.join(parent_dir, corpus_path, file)\n",
    "                sentence = SentenceFetch(current_path).getSentences()\n",
    "                tag = SentenceFetch(current_path).getTags()\n",
    "                sentences.extend(sentence)\n",
    "                tags.extend(tag)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"The folder {corpus_path} was not found in the parent directory.\")\n",
    "    \n",
    "    print('Number of samples: ',len(sentences))\n",
    "    \n",
    "    return sentences, tags\n",
    "\n",
    "def encode_sentence(sentence, tokenizer=tokenizer):\n",
    "    #simply encode sentence using encode_plus\n",
    "    encoded_sent = tokenizer.encode_plus(sentence,\n",
    "                                         is_split_into_words=True,\n",
    "                                         truncation=True,\n",
    "                                         padding='max_length',\n",
    "                                         max_length=50,\n",
    "                                         return_token_type_ids=False,\n",
    "                                         add_special_tokens=False,\n",
    "                                         )\n",
    "    return encoded_sent\n",
    "\n",
    "def align_tags(encoded_sent, tags, tokenizer=tokenizer):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_sent['input_ids'])\n",
    "    aligned_tags = []\n",
    "    counter = 0\n",
    "    for idx, token in enumerate(tokens):\n",
    "        #check if its a sub-word\n",
    "        if token.startswith(\"##\"):\n",
    "            #check previous tag value and copy that\n",
    "            if tags[counter] == 'O':\n",
    "                aligned_tags.append('O')\n",
    "            #if its an entity, then it can't be the beginning so append I-Entity\n",
    "            else:\n",
    "                aligned_tags.append('I-Protein')\n",
    "        \n",
    "        elif token == '[PAD]':\n",
    "            aligned_tags.append('[PAD]')\n",
    "        else:\n",
    "            aligned_tags.append(tags[counter])\n",
    "            #only increment counter if valid token is found\n",
    "            counter+=1\n",
    "    \n",
    "    return aligned_tags\n",
    "\n",
    "def map_tags(alinged_tags, mapping=None):\n",
    "    if not mapping:\n",
    "        label2id = {'O':1, 'B-Protein':2, 'I-Protein':3, '[PAD]':4}\n",
    "    else:\n",
    "        label2id = mapping\n",
    "    def map_labels(label):\n",
    "        return label2id[label]\n",
    "    \n",
    "\n",
    "    mapped_labels = np.vectorize(map_labels)(alinged_tags)\n",
    "\n",
    "    return list(mapped_labels)\n",
    "\n",
    "def merge_tags(encoded_sent, aligned_tags):\n",
    "    #add tags to encoded_sentence dict\n",
    "    if len(encoded_sent['input_ids']) == len(aligned_tags):\n",
    "        encoded_sent['labels'] = aligned_tags\n",
    "    else:\n",
    "        raise ValueError(\"Lengths of sentences and tags do not match.\")\n",
    "    \n",
    "    return encoded_sent\n",
    "\n",
    "def get_faulty_sentences(sentences):\n",
    "    delimiter = '\\t'\n",
    "    #new_sentence = []\n",
    "    faulty_sentences = []\n",
    "    counter = 0\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        for word in sentence:\n",
    "            if delimiter in word:\n",
    "                faulty_sentences.append(idx)\n",
    "    \n",
    "    return set(faulty_sentences)\n",
    "    \n",
    "\n",
    "def tokenize_dataset(sentences, tags, tokenizer=tokenizer):\n",
    "    tokenized_dataset = []\n",
    "    faulty_set = get_faulty_sentences(sentences)\n",
    "    \n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if idx not in faulty_set:\n",
    "            encoded_sent = encode_sentence(sentence, tokenizer)\n",
    "            aligned_tags = align_tags(encoded_sent, tags[idx])\n",
    "            mapped_labels = map_tags(aligned_tags)\n",
    "            final_encoding = merge_tags(encoded_sent, mapped_labels)\n",
    "            tokenized_dataset.append(final_encoding)\n",
    "            \n",
    "    return tokenized_dataset\n",
    "\n",
    "def save_tokenized_data(file_path, data):\n",
    "    with open(file_path, 'wb') as outfile:\n",
    "        pickle.dump(data,outfile)\n",
    "\n",
    "def save_as_tensor_dataset(file_path, data):\n",
    "    #extract input_ids, attention_masks and labels from list of data\n",
    "    input_ids = torch.tensor([d['input_ids'] for d in data])\n",
    "    attention_masks = torch.tensor([d['attention_mask'] for d in data])\n",
    "    labels = torch.tensor([d['labels'] for d in data])\n",
    "    dataset = torch.utils.data.TensorDataset(input_ids, attention_masks,labels)\n",
    "\n",
    "    #save dataset to disk\n",
    "    torch.save(dataset, file_path)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  7462\n",
      "Number of samples:  1448\n",
      "Number of samples:  2446\n"
     ]
    }
   ],
   "source": [
    "PARENT_DIR = 'BioNLP'\n",
    "CORPUS_PATH = 'BioNLP09-IOB'\n",
    "\n",
    "\n",
    "train_sentences, train_tags = extractFromDirectories(parent_dir=PARENT_DIR,\n",
    "                                                     corpus_path=CORPUS_PATH,\n",
    "                                                     file_type='train.tsv')\n",
    "\n",
    "val_sentences, val_tags = extractFromDirectories(parent_dir=PARENT_DIR,\n",
    "                                                     corpus_path=CORPUS_PATH,\n",
    "                                                     file_type='devel.tsv')\n",
    "\n",
    "test_sentences, test_tags = extractFromDirectories(parent_dir=PARENT_DIR,\n",
    "                                                     corpus_path=CORPUS_PATH,\n",
    "                                                     file_type='test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence, tokenizer=tokenizer):\n",
    "    #simply encode sentence using encode_plus\n",
    "    encoded_sent = tokenizer.encode_plus(sentence,\n",
    "                                         is_split_into_words=True,\n",
    "                                         truncation=True,\n",
    "                                         padding='max_length',\n",
    "                                         max_length=50,\n",
    "                                         return_token_type_ids=False,\n",
    "                                         add_special_tokens=False,\n",
    "                                         )\n",
    "    return encoded_sent\n",
    "\n",
    "def align_tags(encoded_sent, tags, tokenizer=tokenizer):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_sent['input_ids'])\n",
    "    aligned_tags = []\n",
    "    counter = 0\n",
    "    for idx, token in enumerate(tokens):\n",
    "        #check if its a sub-word\n",
    "        if token.startswith(\"##\"):\n",
    "            #check previous tag value and copy that\n",
    "            if tags[counter] == 'O':\n",
    "                aligned_tags.append('O')\n",
    "            #if its an entity, then it can't be the beginning so append I-Entity\n",
    "            else:\n",
    "                aligned_tags.append('I-Protein')\n",
    "        \n",
    "        elif token == '[PAD]':\n",
    "            aligned_tags.append('[PAD]')\n",
    "        else:\n",
    "            aligned_tags.append(tags[counter])\n",
    "            #only increment counter if valid token is found\n",
    "            counter+=1\n",
    "    \n",
    "    return aligned_tags\n",
    "\n",
    "def map_tags(alinged_tags, mapping=None):\n",
    "    if not mapping:\n",
    "        label2id = {'O':1, 'B-Protein':2, 'I-Protein':3, '[PAD]':4}\n",
    "    else:\n",
    "        label2id = mapping\n",
    "    def map_labels(label):\n",
    "        return label2id[label]\n",
    "    \n",
    "\n",
    "    mapped_labels = np.vectorize(map_labels)(alinged_tags)\n",
    "\n",
    "    return list(mapped_labels)\n",
    "\n",
    "def merge_tags(encoded_sent, aligned_tags):\n",
    "    #add tags to encoded_sentence dict\n",
    "    if len(encoded_sent['input_ids']) == len(aligned_tags):\n",
    "        encoded_sent['labels'] = aligned_tags\n",
    "    else:\n",
    "        raise ValueError(\"Lengths of sentences and tags do not match.\")\n",
    "    \n",
    "    return encoded_sent\n",
    "\n",
    "def get_faulty_sentences(sentences):\n",
    "    delimiter = '\\t'\n",
    "    #new_sentence = []\n",
    "    faulty_sentences = []\n",
    "    counter = 0\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        for word in sentence:\n",
    "            if delimiter in word:\n",
    "                faulty_sentences.append(idx)\n",
    "    \n",
    "    return set(faulty_sentences)\n",
    "    \n",
    "\n",
    "def tokenize_dataset(sentences, tags, tokenizer=tokenizer):\n",
    "    tokenized_dataset = []\n",
    "    faulty_set = get_faulty_sentences(sentences)\n",
    "    \n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if idx not in faulty_set:\n",
    "            encoded_sent = encode_sentence(sentence, tokenizer)\n",
    "            aligned_tags = align_tags(encoded_sent, tags[idx])\n",
    "            mapped_labels = map_tags(aligned_tags)\n",
    "            final_encoding = merge_tags(encoded_sent, mapped_labels)\n",
    "            tokenized_dataset.append(final_encoding)\n",
    "            \n",
    "    return tokenized_dataset\n",
    "\n",
    "def save_tokenized_data(file_path, data):\n",
    "    with open(file_path, 'wb') as outfile:\n",
    "        pickle.dump(data,outfile)\n",
    "\n",
    "def save_as_tensor_dataset(file_path, data):\n",
    "    #extract input_ids, attention_masks and labels from list of data\n",
    "    input_ids = torch.tensor([d['input_ids'] for d in data])\n",
    "    attention_masks = torch.tensor([d['attention_mask'] for d in data])\n",
    "    labels = torch.tensor([d['labels'] for d in data])\n",
    "    dataset = torch.utils.data.TensorDataset(input_ids, attention_masks,labels)\n",
    "\n",
    "    #save dataset to disk\n",
    "    torch.save(dataset, file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = tokenize_dataset(train_sentences,train_tags,tokenizer)\n",
    "tokenized_val_dataset = tokenize_dataset(val_sentences,val_tags,tokenizer)\n",
    "tokenized_test_dataset = tokenize_dataset(test_sentences,test_tags,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_output = 'prepared_data/train.pickle'\n",
    "val_output = 'prepared_data/val.pickle'\n",
    "test_output = 'prepared_data/test.pickle'\n",
    "\n",
    "save_tokenized_data(train_output, tokenized_train_dataset)\n",
    "save_tokenized_data(val_output, tokenized_val_dataset)\n",
    "save_tokenized_data(test_output, tokenized_test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data back from the file\n",
    "with open('prepared_data/train.pickle', 'rb') as infile:\n",
    "    data = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to a tensor dataset\n",
    "input_ids = torch.tensor([d['input_ids'] for d in data])\n",
    "attention_masks = torch.tensor([d['attention_mask'] for d in data])\n",
    "labels = torch.tensor([d['labels'] for d in data])\n",
    "dataset = torch.utils.data.TensorDataset(input_ids, attention_masks,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dataset,'dummy.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data back from the file\n",
    "with open('train.pickle', 'rb') as infile:\n",
    "    data_loaded = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figuring out all unique tag values\n",
    "tag_values = set()\n",
    "\n",
    "for x in tokenized_train_dataset:\n",
    "    labels = x['labels']\n",
    "    for label in labels:\n",
    "        tag_values.add(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-Protein', 'I-Protein', 'O']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_values = list(set(itertools.chain.from_iterable(train_tags)))\n",
    "\n",
    "tag_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reactive', 'oxygen', 'intermediate', '-', 'dependent', 'NF', '-', 'kappaB', 'activation', 'by', 'interleukin', '-', '1beta', 'requires', '5', '-', 'lipoxygenase', 'or', 'NADPH', 'oxidase', 'activity', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Protein', 'I-Protein', 'I-Protein', 'O', 'B-Protein', 'I-Protein', 'I-Protein', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[0])\n",
    "print(train_tags[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing data and keeping labels intact\n",
    "\n",
    "1. Now we zip sentences and tags \n",
    "2. We tokenize each word; note that some words are broken into sub-words.\n",
    "3. To deal with that, we just extend the label to all subwords generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[226], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_preprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[1;32m      4\u001b[0m MAX_LENGTH \u001b[39m=\u001b[39m \u001b[39m40\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_preprocessing'"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import itertools\n",
    "\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "def tokenizeAndLabelSample(sentence, text_labels):\n",
    "  \"\"\"Generate tokens for words in a text sequence while keeping labels intact. This function does tokenization on the sentence level\"\"\"\n",
    "\n",
    "  tokens = []\n",
    "  labels = []\n",
    "\n",
    "  #zip sentence and label\n",
    "  for word, label in zip(sentence, text_labels):\n",
    "    token = tokenizer.tokenize(word)\n",
    "    tokens.extend(token)\n",
    "\n",
    "    #make sure label is copied for each sub-word IF tokenizer generates sub-words for given word\n",
    "    #len(token) is > 1 if sub-words generated\n",
    "    labels.extend([label]*len(token)) \n",
    "  \n",
    "  return tokens, labels\n",
    "\n",
    "def tokenizeDataset(sentences, labels):\n",
    "  \"\"\"This funciton uses tokenizeAndLabelSample and runs it on the entire dataset\"\"\"\n",
    "  tokenized_text = []\n",
    "  text_labels = []\n",
    "\n",
    "  for i in range(len(sentences)):\n",
    "    sent_tokens, sent_labels = tokenizeAndLabelSample(sentences[i],labels[i])\n",
    "    tokenized_text.append(sent_tokens)\n",
    "    text_labels.append(sent_labels)\n",
    "  \n",
    "  return tokenized_text, text_labels\n",
    "\n",
    "def generateInputIds(tokenized_text):\n",
    "  \"\"\"Generate input_ids\"\"\"\n",
    "  input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(text_sequence) for text_sequence in tokenized_text],\n",
    "                            maxlen = MAX_LENGTH, dtype='long',value=0.0, truncating='post',padding='post')\n",
    "  \n",
    "  return input_ids\n",
    "\n",
    "def generateAttentionMask(input_ids):\n",
    "  \"\"\"Generates attention mask for input ids\"\"\"\n",
    "  attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "\n",
    "  return attention_masks\n",
    "\n",
    "def alignLabels(tags, text_labels):\n",
    "  \"\"\"Ensures that labels match \"\"\"\n",
    "  #get all unique tag values\n",
    "  tag_values = list(set(itertools.chain.from_iterable(tags)))\n",
    "  #append PAD token to tag_values\n",
    "  tag_values.append(\"PAD\")\n",
    "  #create a dictionary mapping tag values to ids\n",
    "  tag_id_dict = {t: i for i,t in enumerate(tag_values)}\n",
    "  #pad \n",
    "  tag_ids = pad_sequences([[tag_id_dict.get(l) for l in lab] for lab in text_labels],\n",
    "                     maxlen=MAX_LENGTH, value=tag_id_dict[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "  \n",
    "  return tag_ids, tag_id_dict\n",
    "\n",
    "def generateInputs(sentences, tags):\n",
    "  \"\"\"A wrapped function that does the whole processing of tokenization and aligning of labels\"\"\"\n",
    "\n",
    "  #tokenize dataset\n",
    "  tokenized_text, text_labels = tokenizeDataset(sentences, tags)\n",
    "\n",
    "  #generate input ids\n",
    "  input_ids = generateInputIds(tokenized_text)\n",
    "\n",
    "  #generate attention mask\n",
    "  attention_masks = generateAttentionMask(input_ids)\n",
    "\n",
    "  #align labels\n",
    "  tag_ids, tag_id_dict = alignLabels(tags, text_labels)\n",
    "\n",
    "  return input_ids, attention_masks, tag_ids, tag_id_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generateInputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[225], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_input_ids, train_attention_masks, train_tag_ids, train_tag_id_dict \u001b[39m=\u001b[39m generateInputs(train_sentences,train_tags)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generateInputs' is not defined"
     ]
    }
   ],
   "source": [
    "train_input_ids, train_attention_masks, train_tag_ids, train_tag_id_dict = generateInputs(train_sentences,train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11336, 19667,  7621,  9533,   118,  7449,   151,  2271,   118,\n",
       "       24181, 13059,  2064, 14915,  1118,  9455,  1513, 17041,  1179,\n",
       "         118,   122, 16632,  1161,  5315,   126,   118,  4764, 10649,\n",
       "        1183,  4915,  6530,  1137,   151, 14569,  2101,  3048,   184,\n",
       "        8745,  9028,  1162,  3246])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text, text_labels = tokenizeDataset(sentences,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets get our input ids \n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import itertools\n",
    "\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(text_sequence) for text_sequence in tokenized_text],\n",
    "                            maxlen = 100, dtype='long',value=0.0, truncating='post',padding='post')\n",
    "\n",
    "tag_values = list(set(itertools.chain.from_iterable(tags)))\n",
    "\n",
    "tag_values.append(\"PAD\")\n",
    "\n",
    "tag2idx = {t: i for i,t in enumerate(tag_values)}\n",
    "\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in text_labels],\n",
    "                     maxlen=100, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate attendtion masks\n",
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tags[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "tr_inputs = torch.tensor(input_ids)\n",
    "tr_tags = torch.tensor(tags)\n",
    "tr_masks = torch.tensor(attention_masks)\n",
    "\n",
    "train_dataset = TensorDataset(tr_inputs,tr_masks,tr_tags)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler = RandomSampler(train_dataset),\n",
    "    batch_size = BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'I-Protein', 'E-Protein', 'S-Protein', 'B-Protein', 'PAD']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\adil.ahmed\\Miniconda3\\envs\\biobert\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, AdamW\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.2', num_labels = len(tag_values))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "#check for cuda\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "# for num, batch in enumerate(train_loader):\n",
    "#     label = batch[2].type(torch.LongTensor).to(device)\n",
    "\n",
    "#     output = model(input_ids = batch[0].to(device), attention_mask = batch[1].to(device), labels = label)\n",
    "#     loss = output[0]\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 6 ========\n",
      "Training...\n",
      "Training loss:  0.11376170058101416\n",
      "\n",
      "======== Epoch 2 / 6 ========\n",
      "Training...\n",
      "Training loss:  0.04043763983175158\n",
      "\n",
      "======== Epoch 3 / 6 ========\n",
      "Training...\n",
      "Training loss:  0.03095139863193035\n",
      "\n",
      "======== Epoch 4 / 6 ========\n",
      "Training...\n",
      "Training loss:  0.02555634008385241\n",
      "\n",
      "======== Epoch 5 / 6 ========\n",
      "Training...\n",
      "Training loss:  0.021814862606860698\n",
      "\n",
      "======== Epoch 6 / 6 ========\n",
      "Training...\n",
      "Training loss:  0.01945531152449548\n"
     ]
    }
   ],
   "source": [
    "#specify number of epochs\n",
    "epochs = 6\n",
    "\n",
    "total_steps = len(train_loader)*epochs\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "\n",
    "for epoch_i in range(0,epochs):\n",
    "\n",
    "    #keeping track of ep\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    #flush out total loss after each epoch\n",
    "    total_train_loss = 0\n",
    "\n",
    "    #put model in training mode\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_label = batch[2].type(torch.LongTensor).to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(input_ids = batch_input_ids,\n",
    "                       attention_mask = batch_attention_mask,\n",
    "                       labels = batch_label)\n",
    "        \n",
    "        loss = output[0]\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        #backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        #clip the norm of gradients to 1.0\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    average_train_loss = total_train_loss / len(train_loader)\n",
    "    print('Training loss: ',average_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/tokenizer_config.json',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/vocab.txt',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the model\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model and tokenizer\n",
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "\n",
    "input_dir = './model_save/'\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(input_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(input_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adil.ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "#Using the model for predictions\n",
    "text = \"\"\"In addition to their essential catalytic role in protein biosynthesis, aminoacyl-tRNA synthetases participate in numerous other functions, including regulation of gene expression and amino acid biosynthesis via transamidation pathways. Herein, we describe a class of aminoacyl-tRNA synthetase-like (HisZ) proteins based on the catalytic core of the contemporary class II histidyl-tRNA synthetase whose members lack aminoacylation activity but are instead essential components of the first enzyme in histidine biosynthesis ATP phosphoribosyltransferase (HisG). Prediction of the function of HisZ in Lactococcus lactis was assisted by comparative genomics, a technique that revealed a link between the presence or the absence of HisZ and a systematic variation in the length of the HisG polypeptide. HisZ is required for histidine prototrophy, and three other lines of evidence support the direct involvement of HisZ in the transferase function. (i) Genetic experiments demonstrate that complementation of an in-frame deletion of HisG from Escherichia coli (which does not possess HisZ) requires both HisG and HisZ from L. lactis. (ii) Coelution of HisG and HisZ during affinity chromatography provides evidence of direct physical interaction. (iii) Both HisG and HisZ are required for catalysis of the ATP phosphoribosyltransferase reaction. This observation of a common protein domain linking amino acid biosynthesis and protein synthesis implies an early connection between the biosynthesis of amino acids and proteins.\"\"\"\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "sent_text = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In addition to their essential catalytic role in protein biosynthesis, aminoacyl-tRNA synthetases participate in numerous other functions, including regulation of gene expression and amino acid biosynthesis via transamidation pathways.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = []\n",
    "for sentence in sent_text:\n",
    "    tokenized_text.append(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['In',\n",
       "  'addition',\n",
       "  'to',\n",
       "  'their',\n",
       "  'essential',\n",
       "  'catalytic',\n",
       "  'role',\n",
       "  'in',\n",
       "  'protein',\n",
       "  'biosynthesis',\n",
       "  ',',\n",
       "  'aminoacyl-tRNA',\n",
       "  'synthetases',\n",
       "  'participate',\n",
       "  'in',\n",
       "  'numerous',\n",
       "  'other',\n",
       "  'functions',\n",
       "  ',',\n",
       "  'including',\n",
       "  'regulation',\n",
       "  'of',\n",
       "  'gene',\n",
       "  'expression',\n",
       "  'and',\n",
       "  'amino',\n",
       "  'acid',\n",
       "  'biosynthesis',\n",
       "  'via',\n",
       "  'transamidation',\n",
       "  'pathways',\n",
       "  '.'],\n",
       " ['Herein',\n",
       "  ',',\n",
       "  'we',\n",
       "  'describe',\n",
       "  'a',\n",
       "  'class',\n",
       "  'of',\n",
       "  'aminoacyl-tRNA',\n",
       "  'synthetase-like',\n",
       "  '(',\n",
       "  'HisZ',\n",
       "  ')',\n",
       "  'proteins',\n",
       "  'based',\n",
       "  'on',\n",
       "  'the',\n",
       "  'catalytic',\n",
       "  'core',\n",
       "  'of',\n",
       "  'the',\n",
       "  'contemporary',\n",
       "  'class',\n",
       "  'II',\n",
       "  'histidyl-tRNA',\n",
       "  'synthetase',\n",
       "  'whose',\n",
       "  'members',\n",
       "  'lack',\n",
       "  'aminoacylation',\n",
       "  'activity',\n",
       "  'but',\n",
       "  'are',\n",
       "  'instead',\n",
       "  'essential',\n",
       "  'components',\n",
       "  'of',\n",
       "  'the',\n",
       "  'first',\n",
       "  'enzyme',\n",
       "  'in',\n",
       "  'histidine',\n",
       "  'biosynthesis',\n",
       "  'ATP',\n",
       "  'phosphoribosyltransferase',\n",
       "  '(',\n",
       "  'HisG',\n",
       "  ')',\n",
       "  '.'],\n",
       " ['Prediction',\n",
       "  'of',\n",
       "  'the',\n",
       "  'function',\n",
       "  'of',\n",
       "  'HisZ',\n",
       "  'in',\n",
       "  'Lactococcus',\n",
       "  'lactis',\n",
       "  'was',\n",
       "  'assisted',\n",
       "  'by',\n",
       "  'comparative',\n",
       "  'genomics',\n",
       "  ',',\n",
       "  'a',\n",
       "  'technique',\n",
       "  'that',\n",
       "  'revealed',\n",
       "  'a',\n",
       "  'link',\n",
       "  'between',\n",
       "  'the',\n",
       "  'presence',\n",
       "  'or',\n",
       "  'the',\n",
       "  'absence',\n",
       "  'of',\n",
       "  'HisZ',\n",
       "  'and',\n",
       "  'a',\n",
       "  'systematic',\n",
       "  'variation',\n",
       "  'in',\n",
       "  'the',\n",
       "  'length',\n",
       "  'of',\n",
       "  'the',\n",
       "  'HisG',\n",
       "  'polypeptide',\n",
       "  '.'],\n",
       " ['HisZ',\n",
       "  'is',\n",
       "  'required',\n",
       "  'for',\n",
       "  'histidine',\n",
       "  'prototrophy',\n",
       "  ',',\n",
       "  'and',\n",
       "  'three',\n",
       "  'other',\n",
       "  'lines',\n",
       "  'of',\n",
       "  'evidence',\n",
       "  'support',\n",
       "  'the',\n",
       "  'direct',\n",
       "  'involvement',\n",
       "  'of',\n",
       "  'HisZ',\n",
       "  'in',\n",
       "  'the',\n",
       "  'transferase',\n",
       "  'function',\n",
       "  '.'],\n",
       " ['(',\n",
       "  'i',\n",
       "  ')',\n",
       "  'Genetic',\n",
       "  'experiments',\n",
       "  'demonstrate',\n",
       "  'that',\n",
       "  'complementation',\n",
       "  'of',\n",
       "  'an',\n",
       "  'in-frame',\n",
       "  'deletion',\n",
       "  'of',\n",
       "  'HisG',\n",
       "  'from',\n",
       "  'Escherichia',\n",
       "  'coli',\n",
       "  '(',\n",
       "  'which',\n",
       "  'does',\n",
       "  'not',\n",
       "  'possess',\n",
       "  'HisZ',\n",
       "  ')',\n",
       "  'requires',\n",
       "  'both',\n",
       "  'HisG',\n",
       "  'and',\n",
       "  'HisZ',\n",
       "  'from',\n",
       "  'L.',\n",
       "  'lactis',\n",
       "  '.'],\n",
       " ['(',\n",
       "  'ii',\n",
       "  ')',\n",
       "  'Coelution',\n",
       "  'of',\n",
       "  'HisG',\n",
       "  'and',\n",
       "  'HisZ',\n",
       "  'during',\n",
       "  'affinity',\n",
       "  'chromatography',\n",
       "  'provides',\n",
       "  'evidence',\n",
       "  'of',\n",
       "  'direct',\n",
       "  'physical',\n",
       "  'interaction',\n",
       "  '.'],\n",
       " ['(',\n",
       "  'iii',\n",
       "  ')',\n",
       "  'Both',\n",
       "  'HisG',\n",
       "  'and',\n",
       "  'HisZ',\n",
       "  'are',\n",
       "  'required',\n",
       "  'for',\n",
       "  'catalysis',\n",
       "  'of',\n",
       "  'the',\n",
       "  'ATP',\n",
       "  'phosphoribosyltransferase',\n",
       "  'reaction',\n",
       "  '.'],\n",
       " ['This',\n",
       "  'observation',\n",
       "  'of',\n",
       "  'a',\n",
       "  'common',\n",
       "  'protein',\n",
       "  'domain',\n",
       "  'linking',\n",
       "  'amino',\n",
       "  'acid',\n",
       "  'biosynthesis',\n",
       "  'and',\n",
       "  'protein',\n",
       "  'synthesis',\n",
       "  'implies',\n",
       "  'an',\n",
       "  'early',\n",
       "  'connection',\n",
       "  'between',\n",
       "  'the',\n",
       "  'biosynthesis',\n",
       "  'of',\n",
       "  'amino',\n",
       "  'acids',\n",
       "  'and',\n",
       "  'proteins',\n",
       "  '.']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = tokenized_text[0]\n",
    "\n",
    "len(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = []\n",
    "\n",
    "for word in example:\n",
    "    tokenized_word = tokenizer.tokenize(word)\n",
    "    tokenized_sentence.extend(tokenized_word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "input_attentions = [[1]*len(input_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sentences = []\n",
    "pred_labels = []\n",
    "\n",
    "for x,y in zip(input_ids,input_attentions):\n",
    "    x = torch.tensor(x).cuda()\n",
    "    y = torch.tensor(y).cuda()\n",
    "    #x = x.view(-1,x.size()[-1])\n",
    "    #y = y.view(-1,y.size()[-1])\n",
    "    with torch.no_grad():\n",
    "        _,y_hat = model(x,y,None)\n",
    "    label_indices = y_hat.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = ['The',\n",
    " 'Cdc6',\n",
    " 'protein',\n",
    " 'is',\n",
    " 'ubiquitinated',\n",
    " 'in',\n",
    " 'vivo',\n",
    " 'for',\n",
    " 'proteolysis',\n",
    " 'in',\n",
    " 'Saccharomyces',\n",
    " 'cerevisiae',\n",
    " '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_example = sent_text[0]\n",
    "\n",
    "text = tokenizer(sentences[1], padding = 'max_length', max_length = 100, truncation=True, return_tensors ='pt')\n",
    "\n",
    "input_id = text['input_ids'].to(device)\n",
    "mask = text['attention_mask'].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_id,mask,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output['logits'][0]\n",
    "predictions = logits.argmax(dim=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biobert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5f2e852b19635869680eb89820f551f0c9ffdb8d4f5703d6a2735a356ddba28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
